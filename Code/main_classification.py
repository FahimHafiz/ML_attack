# -*- coding: utf-8 -*-
"""ML_security_final_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pV1W2n7wPDDeqCEUCYFsBeQTmJ35go6X
"""

# Basic Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro, anderson, pearsonr, spearmanr
import os
from google.colab import files


# Scikit-Learn Utilities
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder

# Machine Learning Models
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier

# Evaluation Metrics
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score, confusion_matrix,
    roc_auc_score, precision_recall_curve, auc, classification_report,
    PrecisionRecallDisplay, RocCurveDisplay, roc_curve
)

# TensorFlow/Keras for Deep Learning Models
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# @title Class for Data preprocessing and splitting

class DataProcessor:
    def __init__(self):
        self.df = None

    def read_data(self, file_path, sheet_name):
        """
        Reads the Excel file and returns the DataFrame.
        """
        self.df = pd.read_excel(file_path, sheet_name=sheet_name)
        print("Dataset Head:")
        print(self.df.head())
        return self.df

    def explore_data(self):
        """
        Displays basic information, descriptive statistics, missing values,
        and plots the distribution of the output variable.
        """
        if self.df is None:
            raise ValueError("DataFrame is not loaded. Call read_data() first.")

        print("\nBasic Information about the Dataset:")
        print(self.df.info())

        print("\nDescriptive Statistics:")
        print(self.df.describe())

        print("\nMissing Values:")
        print(self.df.isnull().sum())

        # Visualize the distribution of the output variable
        plt.figure(figsize=(6, 4))
        sns.countplot(x='Out_simulation_bin', data=self.df)
        plt.title('Distribution of Output Variable (Out_simulation_bin)')
        plt.show()

    def preprocess_data(self):
        """
        Creates a copy of the DataFrame, preprocesses it (converts hexadecimal features to integers),
        and returns the preprocessed DataFrame. The original DataFrame remains unchanged.
        """
        if self.df is None:
            raise ValueError("DataFrame is not loaded. Call read_data() first.")

        # Create a copy of the DataFrame for preprocessing
        df_processed = self.df.copy()

        print("\nDataset Head before conversion:")
        print(df_processed.head())

        # Convert hexadecimal features to integers
        for col in ['R1', 'R2', 'R3', 'R4']:
            df_processed[col] = df_processed[col].astype(str)  # Ensure all values are strings
            df_processed[col] = df_processed[col].apply(lambda x: int(x, 16))

        print("\nDataset Head after conversion:")
        print(df_processed.head())

        return df_processed

    def split_data(self, df_processed, test_size=0.2):
        """
        Splits the dataset into training and test sets and returns them along with printing their info.
        """
        if df_processed is None:
            raise ValueError("DataFrame is not loaded. Call preprocess_data() first.")
        # Separate features and target
        X = df_processed[['R1', 'R2', 'R3', 'R4']]
        y = df_processed['Out_simulation_bin']

        print("\nFeatures (X):")
        print(X.head())

        print("\nTarget (y):")
        print(y.head())

        # Split the dataset into training and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

        # Print the sizes of the training and test sets
        print(f"\nTraining set size: {len(X_train)}")
        print(f"Test set size: {len(X_test)}")

        # Print the first few rows of the training and test sets
        print("\nTraining Set (imbalanced):")
        print(X_train.head())
        print(y_train.head())

        print("\nTest Set (imbalanced):")
        print(X_test.head())
        print(y_test.head())

        return X_train, X_test, y_train, y_test

# @title Class Model selection and training

class BinaryClassifierHytune:
    def __init__(self, model_name='random_forest', tune_hyperparameters=False, X_train=None, y_train=None):
        """Initialize the classifier and optionally perform hyperparameter tuning."""
        self.model_name = model_name
        self.tune_hyperparameters = tune_hyperparameters
        self.X_train = X_train  # Store training data for tuning
        self.y_train = y_train
        self.model = self.select_model(model_name)

    def select_model(self, model_name):
        """Selects a binary classification model with optional hyperparameter tuning."""
        models = {
            'random_forest': (RandomForestClassifier(), {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }),
            'gradient_boosting': (GradientBoostingClassifier(), {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 10]
            }),
            'logistic_regression': (LogisticRegression(), {
                'C': [0.1, 1, 10],
                'solver': ['liblinear', 'lbfgs']
            }),
            'svm': (SVC(probability=True), {
                'C': [0.1, 1, 10],
                'kernel': ['linear', 'rbf']
            }),
            'xgboost': (XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'max_depth': [3, 5, 10]
            }),
            'lightgbm': (LGBMClassifier(), {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.2],
                'num_leaves': [31, 50, 100]
            }),
            'knn': (KNeighborsClassifier(), {
                'n_neighbors': [3, 5, 7, 10]
            }),
            'naive_bayes': (GaussianNB(), {}),
            'decision_tree': (DecisionTreeClassifier(), {
                'max_depth': [None, 10, 20, 30],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            })
        }

        model, param_grid = models.get(model_name, models['random_forest'])

        if self.tune_hyperparameters and param_grid and self.X_train is not None and self.y_train is not None:
            print(f"üîç Performing Hyperparameter Tuning for {model_name}...")
            return self.hyperparameter_tuning(model, param_grid)

        return model

    def hyperparameter_tuning(self, model, param_grid):
        """Performs RandomizedSearchCV to find the best hyperparameters."""
        search = RandomizedSearchCV(
            estimator=model,
            param_distributions=param_grid,
            n_iter=10,
            cv=3,
            scoring='accuracy',
            random_state=42,
            n_jobs=-1
        )
        search.fit(self.X_train, self.y_train)
        print(f"‚úÖ Best Hyperparameters: {search.best_params_}")
        return search.best_estimator_

    def train(self, X_train, y_train):
        """Trains the model."""
        self.X_train, self.y_train = X_train, y_train
        print(f"üöÄ Training {self.model_name} model...")
        self.model.fit(X_train, y_train)
        print("‚úÖ Training complete!")

    def evaluate(self, X, y, dataset_type='Test'):
        """Evaluates the model and prints metrics."""
        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve
        import matplotlib.pyplot as plt
        import seaborn as sns

        y_pred = self.model.predict(X)
        y_prob = self.model.predict_proba(X)[:, 1]

        accuracy = accuracy_score(y, y_pred)
        auc_roc = roc_auc_score(y, y_prob)
        precision, recall, _ = precision_recall_curve(y, y_prob)
        auc_pr = auc(recall, precision)

        print(f"\nüìä {dataset_type} Set Metrics:")
        print(f"Accuracy: {accuracy:.3f}")
        print(f"AUC-ROC: {auc_roc:.3f}")
        print(f"AUC-PR: {auc_pr:.3f}")

        # Plot Confusion Matrix
        plt.figure(figsize=(5, 4))
        sns.heatmap(confusion_matrix(y, y_pred), annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix ({dataset_type} Set)')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.show()

        # Plot Precision-Recall Curve
        plt.figure(figsize=(6, 4))
        plt.plot(recall, precision, marker='.')
        plt.title(f'Precision-Recall Curve ({dataset_type} Set)')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.show()

        # Store results
        self.metrics = {'accuracy': accuracy, 'auc_pr': auc_pr, 'auc_roc': auc_roc}

    def get_metrics(self):
        """Returns stored evaluation metrics."""
        return self.metrics

# @title Class for plotting and saving image

class AUCPlotter:
    def __init__(self, auc_roc_df, save_dir="auc_roc_plots"):
        """
        Initializes the AUCPlotter class.

        Parameters:
        - auc_roc_df: DataFrame containing AUC-ROC scores for different train sizes.
        - save_dir: Directory where plots will be saved.
        """
        self.auc_roc_df = auc_roc_df
        self.save_dir = save_dir
        os.makedirs(self.save_dir, exist_ok=True)  # Create directory if it doesn't exist

    def plot_individual_sets(self):
        """
        Plots, displays, saves, and downloads AUC-ROC vs Train Size plots for each set.
        """
        for set_name in self.auc_roc_df.index:
            plt.figure(figsize=(8, 6))
            train_sizes = self.auc_roc_df.columns.astype(float)  # Convert column names to float
            auc_values = self.auc_roc_df.loc[set_name].astype(float)  # Get AUC-ROC scores

            plt.plot(train_sizes, auc_values, marker='o', linestyle='-', label=f"{set_name}")
            plt.xlabel("Train Size")
            plt.ylabel("AUC-ROC Score")
            plt.title(f"AUC-ROC vs Train Size ({set_name})")
            plt.legend()
            plt.grid(True)

            # Display the plot
            plt.show()

            # Save the plot
            file_path = os.path.join(self.save_dir, f"{set_name}_auc_roc.png")
            plt.savefig(file_path, dpi=600)
            plt.close()

            # Ensure download happens after saving
            files.download(file_path)

    def plot_combined(self):
        """
        Plots, displays, saves, and downloads a single graph with all sets in one plot.
        """
        plt.figure(figsize=(10, 8))

        for set_name in self.auc_roc_df.index:
            train_sizes = self.auc_roc_df.columns.astype(float)  # Convert column names to float
            auc_values = self.auc_roc_df.loc[set_name].astype(float)  # Get AUC-ROC scores

            plt.plot(train_sizes, auc_values, marker='o', linestyle='-', label=set_name)

        plt.xlabel("Train Size")
        plt.ylabel("AUC-ROC Score")
        plt.title("AUC-ROC vs Train Size (All Sets)")
        plt.legend(loc="best")
        plt.grid(True)

        # Display the combined plot
        plt.show()

        # Save the combined plot
        combined_path = os.path.join(self.save_dir, "combined_auc_roc.png")
        plt.savefig(combined_path, dpi=600)
        plt.close()

        # Ensure download happens after saving
        files.download(combined_path)

# @title Main program for training

# Example usage
if __name__ == "__main__":
    # Initialize the DataProcessor class
    processor = DataProcessor()

    # Define test size values
    test_size_arr = np.linspace(0.1, 0.9, 10)  # 10 test sizes from 0.1 to 0.9

    # Define sheet names (assuming they are named "Set1", "Set2", ..., "Set12")
    sheet_names = [f"Set{i}" for i in range(1, 13)]  # 12 sheets

    # Initialize an empty DataFrame to store AUC-ROC scores
    auc_roc_df = pd.DataFrame(index=sheet_names, columns=test_size_arr)

    # File path
    file_path = r'ML_data_set_Statistical_Analysis_0V_0V.xlsx'  # Replace with actual file path

    # Loop through each sheet
    for sheet in sheet_names:
        for test_size in test_size_arr:
            print(f"\nRunning {sheet} for classification with test size {test_size:.2f}!")

            # Read data
            df_input = processor.read_data(file_path, sheet)

            # Explore data
            processor.explore_data()

            # Preprocess data
            df_processed = processor.preprocess_data()

            # Split the data
            print(f"Splitting data for {sheet} at test size {test_size:.2f}")
            X_train, X_test, y_train, y_test = processor.split_data(df_processed, test_size=test_size)

            print(f"Running classifier for {sheet} at test size {test_size:.2f}")
            # Initialize classifier with hyperparameter tuning
            classifier = BinaryClassifierHytune(model_name='random_forest', tune_hyperparameters=True,
                                                X_train=X_train, y_train=y_train)

            # Train the model
            classifier.train(X_train, y_train)

            # Evaluate on training data
            classifier.evaluate(X_train, y_train, dataset_type='Training')

            # Evaluate on test data
            classifier.evaluate(X_test, y_test, dataset_type='Test')

            # Retrieve metrics
            metrics = classifier.get_metrics()

            # Store the AUC-ROC score in the DataFrame
            auc_roc_df.loc[sheet, test_size] = metrics['auc_roc']

            print(f"Metrics for {sheet} at test size {test_size:.2f}: {metrics}")

    # Display the final DataFrame
    print("\nFinal AUC-ROC DataFrame:")
    print(auc_roc_df)

# @title saving the auc_roc in csv
auc_roc_df.columns = 1 - test_size_arr
auc_roc_df
# Save DataFrame to CSV
auc_roc_df.to_csv("auc_roc_results.csv")

# @title plotting the accuracy score vs train size for each set
# Initialize the plotter
plotter = AUCPlotter(auc_roc_df)

# Plot, display, save, and download individual plots
plotter.plot_individual_sets()

# Plot, display, save, and download the combined plot
plotter.plot_combined()